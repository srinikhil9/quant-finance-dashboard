{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variational Autoencoder for Latent Factor Discovery\n",
        "\n",
        "This notebook demonstrates how to use Variational Autoencoders (VAE) to discover latent factors in stock returns, similar to how PCA finds principal components but with non-linear relationships.\n",
        "\n",
        "## Key Concepts\n",
        "- **Autoencoder**: Neural network that learns compressed representations\n",
        "- **Variational Autoencoder (VAE)**: Probabilistic autoencoder with regularized latent space\n",
        "- **Latent Factors**: Hidden variables that explain cross-sectional return variation\n",
        "\n",
        "## Requirements\n",
        "```bash\n",
        "pip install torch numpy pandas matplotlib yfinance scikit-learn\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate Synthetic Stock Returns with Factor Structure\n",
        "\n",
        "We generate synthetic returns that follow a factor model:\n",
        "$$r_i = \\beta_i^{\\top} f + \\epsilon_i$$\n",
        "\n",
        "where $f$ are latent factors and $\\beta_i$ are factor loadings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_factor_returns(n_stocks=100, n_days=1000, n_factors=5):\n",
        "    \"\"\"Generate synthetic returns with factor structure\"\"\"\n",
        "    # Generate factor returns (with different volatilities)\n",
        "    factor_vols = np.array([0.02, 0.015, 0.01, 0.008, 0.005])[:n_factors]\n",
        "    factors = np.random.randn(n_days, n_factors) * factor_vols\n",
        "    \n",
        "    # Add autocorrelation to factors\n",
        "    for i in range(1, n_days):\n",
        "        factors[i] = 0.1 * factors[i-1] + 0.95 * factors[i]\n",
        "    \n",
        "    # Generate factor loadings (betas) for each stock\n",
        "    # Use industry structure: stocks in same 'sector' have similar loadings\n",
        "    n_sectors = 10\n",
        "    sector_loadings = np.random.randn(n_sectors, n_factors)\n",
        "    stock_sectors = np.random.choice(n_sectors, n_stocks)\n",
        "    \n",
        "    loadings = np.zeros((n_stocks, n_factors))\n",
        "    for i in range(n_stocks):\n",
        "        # Sector loading + stock-specific noise\n",
        "        loadings[i] = sector_loadings[stock_sectors[i]] + np.random.randn(n_factors) * 0.3\n",
        "    \n",
        "    # Generate returns: r = loadings @ factors.T + idiosyncratic\n",
        "    idiosyncratic_vol = np.random.uniform(0.01, 0.03, n_stocks)\n",
        "    idiosyncratic = np.random.randn(n_days, n_stocks) * idiosyncratic_vol\n",
        "    \n",
        "    returns = factors @ loadings.T + idiosyncratic\n",
        "    \n",
        "    # Create DataFrame with stock tickers\n",
        "    tickers = [f'STOCK_{i:03d}' for i in range(n_stocks)]\n",
        "    dates = pd.date_range('2020-01-01', periods=n_days, freq='B')\n",
        "    \n",
        "    df = pd.DataFrame(returns, index=dates, columns=tickers)\n",
        "    \n",
        "    return df, factors, loadings, stock_sectors\n",
        "\n",
        "# Generate data\n",
        "N_STOCKS = 100\n",
        "N_DAYS = 1000\n",
        "N_TRUE_FACTORS = 5\n",
        "\n",
        "returns_df, true_factors, true_loadings, sectors = generate_factor_returns(\n",
        "    N_STOCKS, N_DAYS, N_TRUE_FACTORS\n",
        ")\n",
        "\n",
        "print(f\"Returns shape: {returns_df.shape}\")\n",
        "print(f\"True factors shape: {true_factors.shape}\")\n",
        "print(f\"True loadings shape: {true_loadings.shape}\")\n",
        "print(f\"\\nReturn statistics:\")\n",
        "print(returns_df.describe().T.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. PCA Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize returns\n",
        "scaler = StandardScaler()\n",
        "returns_scaled = scaler.fit_transform(returns_df.values)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=10)\n",
        "pca_factors = pca.fit_transform(returns_scaled)\n",
        "\n",
        "print(\"PCA Explained Variance Ratio:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_[:10]):\n",
        "    print(f\"  PC{i+1}: {var:.4f} ({sum(pca.explained_variance_ratio_[:i+1]):.4f} cumulative)\")\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(range(1, 11), pca.explained_variance_ratio_[:10], alpha=0.7)\n",
        "plt.plot(range(1, 11), np.cumsum(pca.explained_variance_ratio_[:10]), 'ro-', label='Cumulative')\n",
        "plt.axhline(y=0.9, color='gray', linestyle='--', label='90% threshold')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('PCA Explained Variance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Variational Autoencoder (VAE) Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    \"\"\"Variational Autoencoder for factor discovery\"\"\"\n",
        "    def __init__(self, input_dim, latent_dim=5, hidden_dims=[64, 32]):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            encoder_layers.extend([\n",
        "                nn.Linear(prev_dim, h_dim),\n",
        "                nn.BatchNorm1d(h_dim),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ])\n",
        "            prev_dim = h_dim\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        \n",
        "        # Latent space (mean and log-variance)\n",
        "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
        "        \n",
        "        # Decoder\n",
        "        decoder_layers = [nn.Linear(latent_dim, hidden_dims[-1]), nn.LeakyReLU(0.2)]\n",
        "        for i in range(len(hidden_dims) - 1, 0, -1):\n",
        "            decoder_layers.extend([\n",
        "                nn.Linear(hidden_dims[i], hidden_dims[i-1]),\n",
        "                nn.BatchNorm1d(hidden_dims[i-1]),\n",
        "                nn.LeakyReLU(0.2)\n",
        "            ])\n",
        "        decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "    \n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"Reparameterization trick for backprop through sampling\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar, z\n",
        "    \n",
        "    def loss_function(self, x, x_recon, mu, logvar, beta=1.0):\n",
        "        \"\"\"VAE loss = Reconstruction loss + KL divergence\"\"\"\n",
        "        # Reconstruction loss (MSE)\n",
        "        recon_loss = F.mse_loss(x_recon, x, reduction='sum')\n",
        "        \n",
        "        # KL divergence: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        \n",
        "        return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
        "\n",
        "# Initialize VAE\n",
        "LATENT_DIM = 5  # Number of latent factors to discover\n",
        "vae = VAE(N_STOCKS, latent_dim=LATENT_DIM, hidden_dims=[64, 32]).to(device)\n",
        "print(f\"VAE parameters: {sum(p.numel() for p in vae.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_vae(model, data, epochs=200, batch_size=64, lr=0.001, beta=1.0):\n",
        "    \"\"\"Train VAE with annealing beta schedule\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "    \n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "    \n",
        "    losses = {'total': [], 'recon': [], 'kl': []}\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_recon = 0\n",
        "        total_kl = 0\n",
        "        n_batches = 0\n",
        "        \n",
        "        # Beta annealing: start low, increase to target\n",
        "        current_beta = min(beta, beta * (epoch / 50))\n",
        "        \n",
        "        # Mini-batch training\n",
        "        indices = torch.randperm(len(data_tensor))\n",
        "        for i in range(0, len(data_tensor), batch_size):\n",
        "            batch_idx = indices[i:i+batch_size]\n",
        "            x_batch = data_tensor[batch_idx]\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            x_recon, mu, logvar, z = model(x_batch)\n",
        "            loss, recon, kl = model.loss_function(x_batch, x_recon, mu, logvar, current_beta)\n",
        "            \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_recon += recon.item()\n",
        "            total_kl += kl.item()\n",
        "            n_batches += 1\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        losses['total'].append(total_loss / n_batches)\n",
        "        losses['recon'].append(total_recon / n_batches)\n",
        "        losses['kl'].append(total_kl / n_batches)\n",
        "        \n",
        "        if (epoch + 1) % 40 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Total={losses['total'][-1]:.2f}, \"\n",
        "                  f\"Recon={losses['recon'][-1]:.2f}, KL={losses['kl'][-1]:.2f}\")\n",
        "    \n",
        "    return losses\n",
        "\n",
        "# Train\n",
        "print(\"Training VAE...\")\n",
        "losses = train_vae(vae, returns_scaled, epochs=200, beta=0.1)\n",
        "\n",
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "axes[0].plot(losses['total'])\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Total Loss')\n",
        "axes[0].set_title('Total Loss')\n",
        "\n",
        "axes[1].plot(losses['recon'])\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Reconstruction Loss')\n",
        "axes[1].set_title('Reconstruction Loss')\n",
        "\n",
        "axes[2].plot(losses['kl'])\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('KL Divergence')\n",
        "axes[2].set_title('KL Divergence')\n",
        "\n",
        "for ax in axes:\n",
        "    ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Extract and Analyze Latent Factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract latent factors\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    data_tensor = torch.tensor(returns_scaled, dtype=torch.float32).to(device)\n",
        "    mu, logvar = vae.encode(data_tensor)\n",
        "    vae_factors = mu.cpu().numpy()  # Use mean as point estimate\n",
        "\n",
        "print(f\"VAE factors shape: {vae_factors.shape}\")\n",
        "\n",
        "# Compare VAE factors with true factors using correlation\n",
        "print(\"\\nCorrelation between VAE factors and true factors:\")\n",
        "corr_matrix = np.corrcoef(vae_factors.T, true_factors.T)[:LATENT_DIM, LATENT_DIM:]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(np.abs(corr_matrix), cmap='Blues', aspect='auto')\n",
        "plt.colorbar(label='|Correlation|')\n",
        "plt.xlabel('True Factor')\n",
        "plt.ylabel('VAE Factor')\n",
        "plt.title('Correlation: VAE Factors vs True Factors')\n",
        "plt.xticks(range(N_TRUE_FACTORS), [f'True {i+1}' for i in range(N_TRUE_FACTORS)])\n",
        "plt.yticks(range(LATENT_DIM), [f'VAE {i+1}' for i in range(LATENT_DIM)])\n",
        "for i in range(LATENT_DIM):\n",
        "    for j in range(N_TRUE_FACTORS):\n",
        "        plt.text(j, i, f'{corr_matrix[i,j]:.2f}', ha='center', va='center')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Best matching (maximum absolute correlation per VAE factor)\n",
        "print(\"\\nBest factor matches:\")\n",
        "for i in range(LATENT_DIM):\n",
        "    best_match = np.argmax(np.abs(corr_matrix[i]))\n",
        "    print(f\"  VAE Factor {i+1} -> True Factor {best_match+1} (corr={corr_matrix[i, best_match]:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare factor time series\n",
        "fig, axes = plt.subplots(LATENT_DIM, 2, figsize=(14, 3*LATENT_DIM))\n",
        "\n",
        "for i in range(LATENT_DIM):\n",
        "    # VAE factor\n",
        "    axes[i, 0].plot(vae_factors[:200, i], label=f'VAE Factor {i+1}', alpha=0.8)\n",
        "    axes[i, 0].set_ylabel('Factor Value')\n",
        "    axes[i, 0].legend()\n",
        "    axes[i, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Corresponding true factor (best match)\n",
        "    best_match = np.argmax(np.abs(corr_matrix[i]))\n",
        "    sign = np.sign(corr_matrix[i, best_match])  # Align signs\n",
        "    axes[i, 1].plot(sign * true_factors[:200, best_match], \n",
        "                   label=f'True Factor {best_match+1}', alpha=0.8, color='orange')\n",
        "    axes[i, 1].set_ylabel('Factor Value')\n",
        "    axes[i, 1].legend()\n",
        "    axes[i, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[-1, 0].set_xlabel('Time')\n",
        "axes[-1, 1].set_xlabel('Time')\n",
        "plt.suptitle('VAE Factors vs True Factors (First 200 days)', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Factor Loadings Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract decoder weights as factor loadings\n",
        "# The first layer of decoder maps latent -> hidden, we want the final mapping\n",
        "with torch.no_grad():\n",
        "    # Get effective loadings by passing unit vectors through decoder\n",
        "    eye = torch.eye(LATENT_DIM).to(device)\n",
        "    decoded = vae.decode(eye).cpu().numpy()\n",
        "    vae_loadings = decoded.T  # (n_stocks, latent_dim)\n",
        "\n",
        "# Compare with true loadings\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# VAE loadings\n",
        "im1 = axes[0].imshow(vae_loadings, cmap='RdBu', aspect='auto')\n",
        "axes[0].set_xlabel('VAE Factor')\n",
        "axes[0].set_ylabel('Stock')\n",
        "axes[0].set_title('VAE Factor Loadings')\n",
        "plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "# True loadings\n",
        "im2 = axes[1].imshow(true_loadings, cmap='RdBu', aspect='auto')\n",
        "axes[1].set_xlabel('True Factor')\n",
        "axes[1].set_ylabel('Stock')\n",
        "axes[1].set_title('True Factor Loadings')\n",
        "plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Reconstruction Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare reconstruction quality: VAE vs PCA\n",
        "vae.eval()\n",
        "with torch.no_grad():\n",
        "    data_tensor = torch.tensor(returns_scaled, dtype=torch.float32).to(device)\n",
        "    x_recon_vae, _, _, _ = vae(data_tensor)\n",
        "    x_recon_vae = x_recon_vae.cpu().numpy()\n",
        "\n",
        "# PCA reconstruction with same number of components\n",
        "pca_5 = PCA(n_components=LATENT_DIM)\n",
        "pca_factors_5 = pca_5.fit_transform(returns_scaled)\n",
        "x_recon_pca = pca_5.inverse_transform(pca_factors_5)\n",
        "\n",
        "# Calculate reconstruction errors\n",
        "mse_vae = np.mean((returns_scaled - x_recon_vae) ** 2)\n",
        "mse_pca = np.mean((returns_scaled - x_recon_pca) ** 2)\n",
        "\n",
        "# Explained variance\n",
        "var_original = np.var(returns_scaled)\n",
        "var_explained_vae = 1 - mse_vae / var_original\n",
        "var_explained_pca = 1 - mse_pca / var_original\n",
        "\n",
        "print(f\"\\nReconstruction Comparison ({LATENT_DIM} factors):\")\n",
        "print(f\"  VAE MSE: {mse_vae:.6f}, Variance Explained: {var_explained_vae:.4f}\")\n",
        "print(f\"  PCA MSE: {mse_pca:.6f}, Variance Explained: {var_explained_pca:.4f}\")\n",
        "\n",
        "# Visualize reconstruction for a few stocks\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
        "stocks_to_show = [0, 25, 50, 75]\n",
        "days_to_show = 100\n",
        "\n",
        "for ax, stock_idx in zip(axes.flat, stocks_to_show):\n",
        "    ax.plot(returns_scaled[:days_to_show, stock_idx], label='Original', alpha=0.8)\n",
        "    ax.plot(x_recon_vae[:days_to_show, stock_idx], label='VAE', alpha=0.8)\n",
        "    ax.plot(x_recon_pca[:days_to_show, stock_idx], label='PCA', alpha=0.8)\n",
        "    ax.set_title(f'Stock {stock_idx}')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Reconstruction Comparison: Original vs VAE vs PCA')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Generate New Samples from Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample from latent space to generate new return scenarios\n",
        "vae.eval()\n",
        "n_samples = 500\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Sample from standard normal (prior)\n",
        "    z_samples = torch.randn(n_samples, LATENT_DIM).to(device)\n",
        "    generated_returns = vae.decode(z_samples).cpu().numpy()\n",
        "\n",
        "# Compare statistics\n",
        "print(\"Return Statistics Comparison:\")\n",
        "print(f\"{'Metric':<20} {'Original':<15} {'Generated':<15}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Mean':<20} {returns_scaled.mean():.6f}     {generated_returns.mean():.6f}\")\n",
        "print(f\"{'Std':<20} {returns_scaled.std():.6f}     {generated_returns.std():.6f}\")\n",
        "print(f\"{'Min':<20} {returns_scaled.min():.6f}    {generated_returns.min():.6f}\")\n",
        "print(f\"{'Max':<20} {returns_scaled.max():.6f}     {generated_returns.max():.6f}\")\n",
        "\n",
        "# Plot distribution comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].hist(returns_scaled.flatten(), bins=50, alpha=0.7, density=True, label='Original')\n",
        "axes[0].hist(generated_returns.flatten(), bins=50, alpha=0.7, density=True, label='Generated')\n",
        "axes[0].set_xlabel('Return')\n",
        "axes[0].set_ylabel('Density')\n",
        "axes[0].set_title('Return Distribution')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Cross-sectional correlation comparison\n",
        "orig_corr = np.corrcoef(returns_scaled[:, :20].T)\n",
        "gen_corr = np.corrcoef(generated_returns[:, :20].T)\n",
        "\n",
        "axes[1].scatter(orig_corr.flatten(), gen_corr.flatten(), alpha=0.5, s=10)\n",
        "axes[1].plot([-1, 1], [-1, 1], 'r--', label='Perfect match')\n",
        "axes[1].set_xlabel('Original Correlation')\n",
        "axes[1].set_ylabel('Generated Correlation')\n",
        "axes[1].set_title('Cross-Sectional Correlations')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Variational Autoencoder (VAE)**: Probabilistic encoder-decoder for factor discovery\n",
        "2. **Latent Factor Extraction**: Using encoder to map returns to factor space\n",
        "3. **Comparison with PCA**: VAE can capture non-linear relationships\n",
        "4. **Generative Capability**: Sample new return scenarios from learned distribution\n",
        "\n",
        "### Key Insights:\n",
        "- VAE learns factors that correlate with true underlying factors\n",
        "- Regularized latent space allows for meaningful interpolation/sampling\n",
        "- Non-linear decoder can capture complex stock-factor relationships\n",
        "\n",
        "### Extensions to Try:\n",
        "- Use real stock returns from yfinance\n",
        "- Add conditional VAE (CVAE) with sector/industry labels\n",
        "- Implement beta-VAE for more disentangled factors\n",
        "- Use temporal VAE (T-VAE) to capture time dynamics"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
