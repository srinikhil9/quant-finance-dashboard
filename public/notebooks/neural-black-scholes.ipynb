{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Black-Scholes: Physics-Informed Neural Networks for Option Pricing\n",
        "\n",
        "This notebook demonstrates how to train a neural network to price options while respecting the Black-Scholes PDE constraints (Physics-Informed Neural Networks - PINNs).\n",
        "\n",
        "## Key Concepts\n",
        "- **Standard NN**: Learn option prices from data alone\n",
        "- **Physics-Informed NN**: Add PDE residual as a loss term, enforcing BS equation\n",
        "- **Automatic Differentiation**: Use PyTorch autograd to compute Greeks automatically\n",
        "\n",
        "## Requirements\n",
        "```bash\n",
        "pip install torch numpy matplotlib scipy\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Black-Scholes Analytical Solution (Ground Truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def black_scholes_call(S, K, T, r, sigma):\n",
        "    \"\"\"Analytical Black-Scholes call option price\"\"\"\n",
        "    if T <= 0:\n",
        "        return max(S - K, 0)\n",
        "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
        "    d2 = d1 - sigma * np.sqrt(T)\n",
        "    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
        "\n",
        "def black_scholes_delta(S, K, T, r, sigma):\n",
        "    \"\"\"Analytical Black-Scholes delta\"\"\"\n",
        "    if T <= 0:\n",
        "        return 1.0 if S > K else 0.0\n",
        "    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
        "    return norm.cdf(d1)\n",
        "\n",
        "# Test analytical solution\n",
        "S, K, T, r, sigma = 100, 100, 1.0, 0.05, 0.2\n",
        "print(f\"BS Call Price: ${black_scholes_call(S, K, T, r, sigma):.4f}\")\n",
        "print(f\"BS Delta: {black_scholes_delta(S, K, T, r, sigma):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_training_data(n_samples=10000):\n",
        "    \"\"\"Generate synthetic option pricing data\"\"\"\n",
        "    # Sample parameters from reasonable ranges\n",
        "    S = np.random.uniform(50, 150, n_samples)      # Stock price\n",
        "    K = np.random.uniform(50, 150, n_samples)      # Strike\n",
        "    T = np.random.uniform(0.01, 2.0, n_samples)    # Time to maturity\n",
        "    r = np.random.uniform(0.01, 0.10, n_samples)   # Risk-free rate\n",
        "    sigma = np.random.uniform(0.1, 0.5, n_samples) # Volatility\n",
        "    \n",
        "    # Calculate BS prices\n",
        "    prices = np.array([black_scholes_call(s, k, t, ri, sig) \n",
        "                       for s, k, t, ri, sig in zip(S, K, T, r, sigma)])\n",
        "    \n",
        "    # Normalize inputs for better training\n",
        "    X = np.column_stack([S/100, K/100, T, r*10, sigma*2])  # Rough normalization\n",
        "    y = prices / 100  # Normalize prices\n",
        "    \n",
        "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_train, y_train = generate_training_data(50000)\n",
        "X_test, y_test = generate_training_data(5000)\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Standard Neural Network for Option Pricing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptionPricingNN(nn.Module):\n",
        "    \"\"\"Standard feedforward neural network for option pricing\"\"\"\n",
        "    def __init__(self, input_dim=5, hidden_dims=[64, 64, 32]):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(hidden_dim)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        layers.append(nn.Softplus())  # Ensure positive prices\n",
        "        self.network = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Train standard NN\n",
        "model_standard = OptionPricingNN().to(device)\n",
        "optimizer = torch.optim.Adam(model_standard.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "X_train_dev = X_train.to(device)\n",
        "y_train_dev = y_train.to(device)\n",
        "\n",
        "losses = []\n",
        "for epoch in range(200):\n",
        "    model_standard.train()\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model_standard(X_train_dev)\n",
        "    loss = criterion(y_pred, y_train_dev)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.6f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Standard NN Training Loss')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Physics-Informed Neural Network (PINN)\n",
        "\n",
        "The Black-Scholes PDE for a call option is:\n",
        "\n",
        "$$\\frac{\\partial V}{\\partial t} + \\frac{1}{2}\\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} + rS\\frac{\\partial V}{\\partial S} - rV = 0$$\n",
        "\n",
        "We add the PDE residual as a regularization term in the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PhysicsInformedOptionNN(nn.Module):\n",
        "    \"\"\"PINN that respects Black-Scholes PDE\"\"\"\n",
        "    def __init__(self, hidden_dims=[64, 64, 32]):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prev_dim = 5  # S, K, T, r, sigma\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.Tanh()  # Smooth activation for better gradients\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "    \n",
        "    def compute_pde_residual(self, S, K, T, r, sigma):\n",
        "        \"\"\"Compute Black-Scholes PDE residual using automatic differentiation\"\"\"\n",
        "        # Enable gradients for inputs\n",
        "        S = S.requires_grad_(True)\n",
        "        T = T.requires_grad_(True)\n",
        "        \n",
        "        # Forward pass\n",
        "        x = torch.stack([S/100, K/100, T, r*10, sigma*2], dim=1)\n",
        "        V = self.network(x) * 100  # Denormalize\n",
        "        \n",
        "        # Compute gradients\n",
        "        dV_dS = torch.autograd.grad(V.sum(), S, create_graph=True)[0]\n",
        "        dV_dT = torch.autograd.grad(V.sum(), T, create_graph=True)[0]\n",
        "        d2V_dS2 = torch.autograd.grad(dV_dS.sum(), S, create_graph=True)[0]\n",
        "        \n",
        "        # BS PDE: dV/dT + 0.5*sigma^2*S^2*d2V/dS2 + r*S*dV/dS - r*V = 0\n",
        "        # Note: We use -dV/dT since T is time to maturity (decreasing)\n",
        "        residual = -dV_dT + 0.5 * sigma**2 * S**2 * d2V_dS2 + r * S * dV_dS - r * V.squeeze()\n",
        "        \n",
        "        return residual\n",
        "\n",
        "# Train PINN\n",
        "model_pinn = PhysicsInformedOptionNN().to(device)\n",
        "optimizer_pinn = torch.optim.Adam(model_pinn.parameters(), lr=0.001)\n",
        "\n",
        "def train_pinn(model, optimizer, X, y, n_epochs=200, pde_weight=0.1):\n",
        "    losses_data = []\n",
        "    losses_pde = []\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Data loss\n",
        "        y_pred = model(X)\n",
        "        loss_data = criterion(y_pred, y)\n",
        "        \n",
        "        # PDE loss (sample subset for efficiency)\n",
        "        n_pde = min(1000, len(X))\n",
        "        idx = torch.randperm(len(X))[:n_pde]\n",
        "        S_pde = X[idx, 0] * 100\n",
        "        K_pde = X[idx, 1] * 100\n",
        "        T_pde = X[idx, 2]\n",
        "        r_pde = X[idx, 3] / 10\n",
        "        sigma_pde = X[idx, 4] / 2\n",
        "        \n",
        "        residual = model.compute_pde_residual(S_pde, K_pde, T_pde, r_pde, sigma_pde)\n",
        "        loss_pde = (residual ** 2).mean()\n",
        "        \n",
        "        # Combined loss\n",
        "        loss = loss_data + pde_weight * loss_pde\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses_data.append(loss_data.item())\n",
        "        losses_pde.append(loss_pde.item())\n",
        "        \n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Data Loss = {loss_data.item():.6f}, PDE Loss = {loss_pde.item():.6f}\")\n",
        "    \n",
        "    return losses_data, losses_pde\n",
        "\n",
        "losses_data, losses_pde = train_pinn(model_pinn, optimizer_pinn, X_train_dev, y_train_dev)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "axes[0].plot(losses_data)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Data MSE Loss')\n",
        "axes[0].set_title('PINN Data Loss')\n",
        "axes[0].set_yscale('log')\n",
        "\n",
        "axes[1].plot(losses_pde)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('PDE Residual')\n",
        "axes[1].set_title('PINN Physics Loss')\n",
        "axes[1].set_yscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare Models and Compute Greeks via Autodiff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_greeks_autodiff(model, S, K, T, r, sigma):\n",
        "    \"\"\"Compute option Greeks using automatic differentiation\"\"\"\n",
        "    S_t = torch.tensor([S/100], requires_grad=True, dtype=torch.float32).to(device)\n",
        "    K_t = torch.tensor([K/100], dtype=torch.float32).to(device)\n",
        "    T_t = torch.tensor([T], requires_grad=True, dtype=torch.float32).to(device)\n",
        "    r_t = torch.tensor([r*10], dtype=torch.float32).to(device)\n",
        "    sigma_t = torch.tensor([sigma*2], requires_grad=True, dtype=torch.float32).to(device)\n",
        "    \n",
        "    x = torch.stack([S_t, K_t, T_t, r_t, sigma_t], dim=1)\n",
        "    V = model(x) * 100  # Denormalize\n",
        "    \n",
        "    # Delta = dV/dS\n",
        "    delta = torch.autograd.grad(V, S_t, create_graph=True)[0] / 100\n",
        "    \n",
        "    # Gamma = d2V/dS2\n",
        "    gamma = torch.autograd.grad(delta, S_t, create_graph=True)[0] / 100\n",
        "    \n",
        "    # Theta = -dV/dT (negative because T is time to maturity)\n",
        "    theta = -torch.autograd.grad(V, T_t)[0]\n",
        "    \n",
        "    # Vega = dV/dsigma\n",
        "    vega = torch.autograd.grad(V, sigma_t)[0] / 2\n",
        "    \n",
        "    return {\n",
        "        'price': V.item(),\n",
        "        'delta': delta.item(),\n",
        "        'gamma': gamma.item(),\n",
        "        'theta': theta.item() / 365,  # Daily theta\n",
        "        'vega': vega.item() / 100  # Per 1% vol change\n",
        "    }\n",
        "\n",
        "# Compare models\n",
        "test_params = {'S': 100, 'K': 100, 'T': 0.5, 'r': 0.05, 'sigma': 0.2}\n",
        "\n",
        "bs_price = black_scholes_call(**test_params)\n",
        "bs_delta = black_scholes_delta(**test_params)\n",
        "\n",
        "model_standard.eval()\n",
        "model_pinn.eval()\n",
        "\n",
        "greeks_standard = compute_greeks_autodiff(model_standard, **test_params)\n",
        "greeks_pinn = compute_greeks_autodiff(model_pinn, **test_params)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON: S=100, K=100, T=0.5y, r=5%, sigma=20%\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<15} {'BS Analytical':<15} {'Standard NN':<15} {'PINN':<15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'Price':<15} ${bs_price:<14.4f} ${greeks_standard['price']:<14.4f} ${greeks_pinn['price']:<14.4f}\")\n",
        "print(f\"{'Delta':<15} {bs_delta:<15.4f} {greeks_standard['delta']:<15.4f} {greeks_pinn['delta']:<15.4f}\")\n",
        "print(f\"{'Gamma':<15} {'N/A':<15} {greeks_standard['gamma']:<15.4f} {greeks_pinn['gamma']:<15.4f}\")\n",
        "print(f\"{'Theta (daily)':<15} {'N/A':<15} {greeks_standard['theta']:<15.4f} {greeks_pinn['theta']:<15.4f}\")\n",
        "print(f\"{'Vega':<15} {'N/A':<15} {greeks_standard['vega']:<15.4f} {greeks_pinn['vega']:<15.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Option Price Surface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create meshgrid for visualization\n",
        "S_range = np.linspace(70, 130, 50)\n",
        "T_range = np.linspace(0.05, 1.0, 50)\n",
        "S_grid, T_grid = np.meshgrid(S_range, T_range)\n",
        "\n",
        "# Compute prices for each model\n",
        "K, r, sigma = 100, 0.05, 0.2\n",
        "\n",
        "# Analytical BS\n",
        "bs_prices = np.array([[black_scholes_call(s, K, t, r, sigma) for s in S_range] for t in T_range])\n",
        "\n",
        "# Neural network predictions\n",
        "with torch.no_grad():\n",
        "    X_viz = torch.tensor(np.column_stack([\n",
        "        S_grid.flatten()/100,\n",
        "        np.full(S_grid.size, K/100),\n",
        "        T_grid.flatten(),\n",
        "        np.full(S_grid.size, r*10),\n",
        "        np.full(S_grid.size, sigma*2)\n",
        "    ]), dtype=torch.float32).to(device)\n",
        "    \n",
        "    pinn_prices = model_pinn(X_viz).cpu().numpy().reshape(S_grid.shape) * 100\n",
        "\n",
        "# Plot\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "ax1.plot_surface(S_grid, T_grid, bs_prices, cmap='viridis', alpha=0.8)\n",
        "ax1.set_xlabel('Stock Price')\n",
        "ax1.set_ylabel('Time to Maturity')\n",
        "ax1.set_zlabel('Option Price')\n",
        "ax1.set_title('Black-Scholes Analytical')\n",
        "\n",
        "ax2 = fig.add_subplot(132, projection='3d')\n",
        "ax2.plot_surface(S_grid, T_grid, pinn_prices, cmap='viridis', alpha=0.8)\n",
        "ax2.set_xlabel('Stock Price')\n",
        "ax2.set_ylabel('Time to Maturity')\n",
        "ax2.set_zlabel('Option Price')\n",
        "ax2.set_title('PINN Prediction')\n",
        "\n",
        "ax3 = fig.add_subplot(133, projection='3d')\n",
        "error = np.abs(pinn_prices - bs_prices)\n",
        "ax3.plot_surface(S_grid, T_grid, error, cmap='hot', alpha=0.8)\n",
        "ax3.set_xlabel('Stock Price')\n",
        "ax3.set_ylabel('Time to Maturity')\n",
        "ax3.set_zlabel('Absolute Error')\n",
        "ax3.set_title('PINN Error vs BS')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMean Absolute Error (PINN vs BS): ${error.mean():.4f}\")\n",
        "print(f\"Max Absolute Error (PINN vs BS): ${error.max():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Standard Neural Network**: Learns option prices from data alone\n",
        "2. **Physics-Informed Neural Network (PINN)**: Incorporates Black-Scholes PDE as a regularization term\n",
        "3. **Automatic Differentiation**: Computes Greeks (Delta, Gamma, Theta, Vega) automatically from the network\n",
        "\n",
        "### Key Advantages of PINNs for Finance:\n",
        "- **Better Extrapolation**: PDE constraint helps in regions with sparse training data\n",
        "- **Interpretability**: Network respects known financial physics\n",
        "- **Consistent Greeks**: Derivatives are mathematically consistent with the pricing function\n",
        "\n",
        "### Extensions to Try:\n",
        "- Add jump-diffusion or stochastic volatility models\n",
        "- Train on real market option prices (implied vol surfaces)\n",
        "- Use boundary conditions (payoff at T=0) as additional constraints"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
