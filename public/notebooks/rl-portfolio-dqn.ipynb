{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Q-Network (DQN) for Portfolio Management\n",
        "\n",
        "This notebook demonstrates how to use Deep Reinforcement Learning for portfolio allocation decisions.\n",
        "\n",
        "## Key Concepts\n",
        "- **DQN (Deep Q-Network)**: Neural network that approximates Q-values for continuous state spaces\n",
        "- **Experience Replay**: Breaks correlations in training data for stable learning\n",
        "- **Target Network**: Stabilizes Q-learning with a slowly-updated target\n",
        "- **Portfolio Environment**: State = market features, Action = allocation weights\n",
        "\n",
        "## Requirements\n",
        "```bash\n",
        "pip install torch numpy pandas matplotlib gym\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Portfolio Trading Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PortfolioEnv:\n",
        "    \"\"\"Trading environment for portfolio allocation\"\"\"\n",
        "    \n",
        "    def __init__(self, returns, lookback=20, transaction_cost=0.001):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            returns: DataFrame of asset returns (n_days x n_assets)\n",
        "            lookback: Number of days for state features\n",
        "            transaction_cost: Cost per dollar traded\n",
        "        \"\"\"\n",
        "        self.returns = returns.values\n",
        "        self.n_assets = returns.shape[1]\n",
        "        self.lookback = lookback\n",
        "        self.transaction_cost = transaction_cost\n",
        "        \n",
        "        # Action space: discrete allocation choices\n",
        "        # 0: Equal weight, 1: Momentum tilt, 2: Low-vol tilt, 3: Cash heavy\n",
        "        self.n_actions = 4\n",
        "        \n",
        "        # State features per asset: returns, volatility, momentum\n",
        "        self.state_dim = self.n_assets * 3 + 2  # + portfolio value, cash\n",
        "        \n",
        "        self.reset()\n",
        "    \n",
        "    def _get_allocation(self, action):\n",
        "        \"\"\"Convert discrete action to portfolio weights\"\"\"\n",
        "        if action == 0:  # Equal weight\n",
        "            weights = np.ones(self.n_assets) / self.n_assets\n",
        "        elif action == 1:  # Momentum tilt (overweight positive momentum)\n",
        "            momentum = self.returns[self.t-self.lookback:self.t].mean(axis=0)\n",
        "            weights = np.maximum(momentum, 0)\n",
        "            weights = weights / (weights.sum() + 1e-8)\n",
        "        elif action == 2:  # Low-vol tilt\n",
        "            vol = self.returns[self.t-self.lookback:self.t].std(axis=0)\n",
        "            weights = 1 / (vol + 1e-8)\n",
        "            weights = weights / weights.sum()\n",
        "        else:  # Cash heavy (25% each asset)\n",
        "            weights = np.ones(self.n_assets) * 0.25 / self.n_assets\n",
        "        \n",
        "        return weights\n",
        "    \n",
        "    def _get_state(self):\n",
        "        \"\"\"Compute state features\"\"\"\n",
        "        recent_returns = self.returns[self.t-self.lookback:self.t]\n",
        "        \n",
        "        # Features for each asset\n",
        "        avg_returns = recent_returns.mean(axis=0)\n",
        "        volatilities = recent_returns.std(axis=0)\n",
        "        momentum = recent_returns.sum(axis=0)  # Cumulative return\n",
        "        \n",
        "        # Normalize features\n",
        "        avg_returns = avg_returns / (np.abs(avg_returns).max() + 1e-8)\n",
        "        volatilities = volatilities / (volatilities.max() + 1e-8)\n",
        "        momentum = momentum / (np.abs(momentum).max() + 1e-8)\n",
        "        \n",
        "        state = np.concatenate([\n",
        "            avg_returns,\n",
        "            volatilities,\n",
        "            momentum,\n",
        "            [self.portfolio_value / 100 - 1],  # Normalized portfolio value\n",
        "            [self.current_weights.sum()]  # Investment level\n",
        "        ])\n",
        "        \n",
        "        return state.astype(np.float32)\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment\"\"\"\n",
        "        self.t = self.lookback\n",
        "        self.portfolio_value = 100.0\n",
        "        self.current_weights = np.ones(self.n_assets) / self.n_assets\n",
        "        return self._get_state()\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Take action and return next state, reward, done\"\"\"\n",
        "        # Get new target weights\n",
        "        new_weights = self._get_allocation(action)\n",
        "        \n",
        "        # Transaction cost\n",
        "        turnover = np.abs(new_weights - self.current_weights).sum()\n",
        "        cost = turnover * self.transaction_cost * self.portfolio_value\n",
        "        \n",
        "        # Apply returns\n",
        "        daily_return = (new_weights * self.returns[self.t]).sum()\n",
        "        self.portfolio_value *= (1 + daily_return)\n",
        "        self.portfolio_value -= cost\n",
        "        \n",
        "        # Update state\n",
        "        self.current_weights = new_weights\n",
        "        self.t += 1\n",
        "        \n",
        "        # Reward: risk-adjusted return (simplified Sharpe)\n",
        "        reward = daily_return * 100 - cost / self.portfolio_value  # Scale for learning\n",
        "        \n",
        "        # Done if end of data\n",
        "        done = self.t >= len(self.returns) - 1\n",
        "        \n",
        "        return self._get_state(), reward, done, {'portfolio_value': self.portfolio_value}\n",
        "\n",
        "# Generate synthetic returns data\n",
        "def generate_market_data(n_days=1000, n_assets=4):\n",
        "    \"\"\"Generate synthetic multi-asset returns with realistic properties\"\"\"\n",
        "    # Base volatilities\n",
        "    vols = np.array([0.15, 0.20, 0.25, 0.12])[:n_assets] / np.sqrt(252)\n",
        "    \n",
        "    # Correlation matrix\n",
        "    corr = np.array([\n",
        "        [1.0, 0.6, 0.4, -0.2],\n",
        "        [0.6, 1.0, 0.5, -0.1],\n",
        "        [0.4, 0.5, 1.0, 0.0],\n",
        "        [-0.2, -0.1, 0.0, 1.0]\n",
        "    ])[:n_assets, :n_assets]\n",
        "    \n",
        "    # Cholesky decomposition for correlated returns\n",
        "    L = np.linalg.cholesky(corr)\n",
        "    \n",
        "    # Generate returns with volatility clustering\n",
        "    returns = []\n",
        "    vol_state = vols.copy()\n",
        "    \n",
        "    for _ in range(n_days):\n",
        "        # GARCH-like vol dynamics\n",
        "        vol_state = 0.9 * vol_state + 0.1 * vols * (1 + np.random.randn(n_assets) * 0.5)\n",
        "        vol_state = np.clip(vol_state, vols * 0.5, vols * 2)\n",
        "        \n",
        "        # Correlated returns\n",
        "        z = np.random.randn(n_assets)\n",
        "        daily_ret = L @ z * vol_state\n",
        "        \n",
        "        # Add small drift\n",
        "        drift = np.array([0.08, 0.10, 0.06, 0.03])[:n_assets] / 252\n",
        "        daily_ret += drift\n",
        "        \n",
        "        returns.append(daily_ret)\n",
        "    \n",
        "    df = pd.DataFrame(returns, columns=['Stock', 'Growth', 'SmallCap', 'Bonds'][:n_assets])\n",
        "    return df\n",
        "\n",
        "# Create environment\n",
        "returns_df = generate_market_data(1500, 4)\n",
        "env = PortfolioEnv(returns_df, lookback=20, transaction_cost=0.001)\n",
        "\n",
        "print(f\"Returns shape: {returns_df.shape}\")\n",
        "print(f\"State dimension: {env.state_dim}\")\n",
        "print(f\"Number of actions: {env.n_actions}\")\n",
        "print(f\"\\nReturn statistics:\")\n",
        "print((returns_df * 252).describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. DQN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"Deep Q-Network for portfolio allocation\"\"\"\n",
        "    def __init__(self, state_dim, n_actions, hidden_dims=[128, 64]):\n",
        "        super().__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = state_dim\n",
        "        for h_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, h_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.LayerNorm(h_dim)\n",
        "            ])\n",
        "            prev_dim = h_dim\n",
        "        \n",
        "        # Dueling architecture: separate value and advantage streams\n",
        "        self.feature_net = nn.Sequential(*layers)\n",
        "        self.value_stream = nn.Linear(hidden_dims[-1], 1)\n",
        "        self.advantage_stream = nn.Linear(hidden_dims[-1], n_actions)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.feature_net(x)\n",
        "        value = self.value_stream(features)\n",
        "        advantage = self.advantage_stream(features)\n",
        "        \n",
        "        # Combine: Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
        "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
        "        return q_values\n",
        "\n",
        "# Experience replay buffer\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def push(self, *args):\n",
        "        self.buffer.append(Transition(*args))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        transitions = random.sample(self.buffer, batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        return batch\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Initialize networks\n",
        "policy_net = DQN(env.state_dim, env.n_actions).to(device)\n",
        "target_net = DQN(env.state_dim, env.n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "print(f\"DQN parameters: {sum(p.numel() for p in policy_net.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, policy_net, target_net, n_actions, \n",
        "                 lr=1e-3, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=500):\n",
        "        self.policy_net = policy_net\n",
        "        self.target_net = target_net\n",
        "        self.n_actions = n_actions\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.steps_done = 0\n",
        "        \n",
        "        self.replay_buffer = ReplayBuffer(10000)\n",
        "    \n",
        "    def select_action(self, state, training=True):\n",
        "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
        "                  np.exp(-self.steps_done / self.epsilon_decay)\n",
        "        self.steps_done += 1\n",
        "        \n",
        "        if training and random.random() < epsilon:\n",
        "            return random.randrange(self.n_actions)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "                q_values = self.policy_net(state_tensor)\n",
        "                return q_values.argmax().item()\n",
        "    \n",
        "    def train_step(self, batch_size=64):\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return None\n",
        "        \n",
        "        batch = self.replay_buffer.sample(batch_size)\n",
        "        \n",
        "        states = torch.tensor(np.array(batch.state), dtype=torch.float32).to(device)\n",
        "        actions = torch.tensor(batch.action, dtype=torch.long).unsqueeze(1).to(device)\n",
        "        rewards = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        next_states = torch.tensor(np.array(batch.next_state), dtype=torch.float32).to(device)\n",
        "        dones = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        \n",
        "        # Current Q values\n",
        "        current_q = self.policy_net(states).gather(1, actions)\n",
        "        \n",
        "        # Double DQN: use policy net for action selection, target net for evaluation\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.policy_net(next_states).argmax(1, keepdim=True)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions)\n",
        "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
        "        \n",
        "        # Huber loss\n",
        "        loss = F.smooth_l1_loss(current_q, target_q)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def update_target(self, tau=0.005):\n",
        "        \"\"\"Soft update target network\"\"\"\n",
        "        for target_param, policy_param in zip(self.target_net.parameters(), \n",
        "                                              self.policy_net.parameters()):\n",
        "            target_param.data.copy_(tau * policy_param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "# Initialize agent\n",
        "agent = DQNAgent(policy_net, target_net, env.n_actions)\n",
        "print(\"Agent initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_dqn(env, agent, n_episodes=300, target_update_freq=10):\n",
        "    \"\"\"Train DQN agent\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_values = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            \n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                losses.append(loss)\n",
        "            \n",
        "            agent.update_target()\n",
        "        \n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_values.append(info['portfolio_value'])\n",
        "        \n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-50:])\n",
        "            avg_value = np.mean(episode_values[-50:])\n",
        "            print(f\"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, \"\n",
        "                  f\"Avg Portfolio Value = ${avg_value:.2f}\")\n",
        "    \n",
        "    return episode_rewards, episode_values, losses\n",
        "\n",
        "# Train\n",
        "print(\"Training DQN agent...\")\n",
        "rewards, values, losses = train_dqn(env, agent, n_episodes=300)\n",
        "\n",
        "# Plot training progress\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Smoothed rewards\n",
        "window = 20\n",
        "smoothed_rewards = pd.Series(rewards).rolling(window).mean()\n",
        "axes[0].plot(rewards, alpha=0.3, label='Raw')\n",
        "axes[0].plot(smoothed_rewards, label=f'{window}-episode MA')\n",
        "axes[0].set_xlabel('Episode')\n",
        "axes[0].set_ylabel('Total Reward')\n",
        "axes[0].set_title('Episode Rewards')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Portfolio values\n",
        "smoothed_values = pd.Series(values).rolling(window).mean()\n",
        "axes[1].plot(values, alpha=0.3)\n",
        "axes[1].plot(smoothed_values)\n",
        "axes[1].axhline(y=100, color='gray', linestyle='--', label='Initial')\n",
        "axes[1].set_xlabel('Episode')\n",
        "axes[1].set_ylabel('Final Portfolio Value ($)')\n",
        "axes[1].set_title('Portfolio Performance')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Training loss\n",
        "if losses:\n",
        "    smoothed_losses = pd.Series(losses).rolling(100).mean()\n",
        "    axes[2].plot(losses, alpha=0.1)\n",
        "    axes[2].plot(smoothed_losses)\n",
        "    axes[2].set_xlabel('Training Step')\n",
        "    axes[2].set_ylabel('Loss')\n",
        "    axes[2].set_title('Training Loss')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate Trained Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_agent(env, agent, n_episodes=10):\n",
        "    \"\"\"Evaluate trained agent without exploration\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        values = [100.0]\n",
        "        actions_taken = []\n",
        "        \n",
        "        while not done:\n",
        "            action = agent.select_action(state, training=False)\n",
        "            actions_taken.append(action)\n",
        "            state, _, done, info = env.step(action)\n",
        "            values.append(info['portfolio_value'])\n",
        "        \n",
        "        results.append({\n",
        "            'values': values,\n",
        "            'actions': actions_taken,\n",
        "            'final_value': values[-1],\n",
        "            'total_return': (values[-1] / 100 - 1) * 100\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Evaluate\n",
        "eval_results = evaluate_agent(env, agent, n_episodes=10)\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"{'Episode':<10} {'Final Value':<15} {'Total Return':<15}\")\n",
        "print(\"-\" * 40)\n",
        "for i, res in enumerate(eval_results):\n",
        "    print(f\"{i+1:<10} ${res['final_value']:<14.2f} {res['total_return']:<14.2f}%\")\n",
        "\n",
        "avg_return = np.mean([r['total_return'] for r in eval_results])\n",
        "print(f\"\\nAverage Return: {avg_return:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compare with Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_baseline(env, strategy='equal_weight'):\n",
        "    \"\"\"Run baseline strategy\"\"\"\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    values = [100.0]\n",
        "    \n",
        "    while not done:\n",
        "        if strategy == 'equal_weight':\n",
        "            action = 0\n",
        "        elif strategy == 'momentum':\n",
        "            action = 1\n",
        "        elif strategy == 'low_vol':\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3  # Conservative\n",
        "        \n",
        "        state, _, done, info = env.step(action)\n",
        "        values.append(info['portfolio_value'])\n",
        "    \n",
        "    return values\n",
        "\n",
        "# Run baselines\n",
        "equal_weight_values = run_baseline(env, 'equal_weight')\n",
        "momentum_values = run_baseline(env, 'momentum')\n",
        "low_vol_values = run_baseline(env, 'low_vol')\n",
        "\n",
        "# Run DQN (best of eval episodes)\n",
        "dqn_values = eval_results[np.argmax([r['final_value'] for r in eval_results])]['values']\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(equal_weight_values, label=f'Equal Weight ({(equal_weight_values[-1]/100-1)*100:.1f}%)', alpha=0.8)\n",
        "plt.plot(momentum_values, label=f'Momentum ({(momentum_values[-1]/100-1)*100:.1f}%)', alpha=0.8)\n",
        "plt.plot(low_vol_values, label=f'Low Vol ({(low_vol_values[-1]/100-1)*100:.1f}%)', alpha=0.8)\n",
        "plt.plot(dqn_values, label=f'DQN Agent ({(dqn_values[-1]/100-1)*100:.1f}%)', linewidth=2)\n",
        "\n",
        "plt.xlabel('Trading Day')\n",
        "plt.ylabel('Portfolio Value ($)')\n",
        "plt.title('DQN vs Baseline Strategies')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Performance metrics\n",
        "def calc_metrics(values):\n",
        "    returns = np.diff(values) / values[:-1]\n",
        "    total_return = (values[-1] / values[0] - 1) * 100\n",
        "    sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
        "    max_dd = np.max(1 - np.array(values) / np.maximum.accumulate(values)) * 100\n",
        "    return total_return, sharpe, max_dd\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Performance Metrics\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Strategy':<15} {'Total Return':<15} {'Sharpe':<15} {'Max DD':<15}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for name, vals in [('Equal Weight', equal_weight_values), \n",
        "                    ('Momentum', momentum_values),\n",
        "                    ('Low Vol', low_vol_values),\n",
        "                    ('DQN Agent', dqn_values)]:\n",
        "    ret, sharpe, dd = calc_metrics(vals)\n",
        "    print(f\"{name:<15} {ret:<15.2f}% {sharpe:<15.2f} {dd:<15.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Analyze Agent's Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze action distribution\n",
        "action_names = ['Equal Weight', 'Momentum', 'Low Vol', 'Conservative']\n",
        "all_actions = []\n",
        "for res in eval_results:\n",
        "    all_actions.extend(res['actions'])\n",
        "\n",
        "action_counts = np.bincount(all_actions, minlength=4)\n",
        "action_pcts = action_counts / len(all_actions) * 100\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(action_names, action_pcts, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.ylabel('Frequency (%)')\n",
        "plt.title('DQN Agent Action Distribution')\n",
        "for i, (name, pct) in enumerate(zip(action_names, action_pcts)):\n",
        "    plt.text(i, pct + 1, f'{pct:.1f}%', ha='center')\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAction Distribution:\")\n",
        "for name, pct in zip(action_names, action_pcts):\n",
        "    print(f\"  {name}: {pct:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Q-Value Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze Q-values across different market states\n",
        "def analyze_q_values(env, agent, n_samples=100):\n",
        "    \"\"\"Collect Q-values across random states\"\"\"\n",
        "    q_values_list = []\n",
        "    states_list = []\n",
        "    \n",
        "    state = env.reset()\n",
        "    for _ in range(n_samples):\n",
        "        action = random.randrange(env.n_actions)\n",
        "        next_state, _, done, _ = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            continue\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            q_values = policy_net(state_tensor).cpu().numpy()[0]\n",
        "        \n",
        "        q_values_list.append(q_values)\n",
        "        states_list.append(state)\n",
        "        state = next_state\n",
        "    \n",
        "    return np.array(q_values_list), np.array(states_list)\n",
        "\n",
        "q_values, states = analyze_q_values(env, agent, 500)\n",
        "\n",
        "# Plot Q-value distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Q-value distribution by action\n",
        "axes[0].boxplot([q_values[:, i] for i in range(4)], labels=action_names)\n",
        "axes[0].set_ylabel('Q-Value')\n",
        "axes[0].set_title('Q-Value Distribution by Action')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-values vs market volatility (using one of the state features)\n",
        "vol_feature = states[:, env.n_assets:2*env.n_assets].mean(axis=1)  # Avg volatility\n",
        "best_actions = q_values.argmax(axis=1)\n",
        "\n",
        "for action in range(4):\n",
        "    mask = best_actions == action\n",
        "    if mask.sum() > 0:\n",
        "        axes[1].scatter(vol_feature[mask], q_values[mask, action], \n",
        "                       alpha=0.5, label=action_names[action], s=20)\n",
        "\n",
        "axes[1].set_xlabel('Average Volatility (normalized)')\n",
        "axes[1].set_ylabel('Q-Value')\n",
        "axes[1].set_title('Q-Values vs Market Volatility')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Portfolio Environment**: Custom gym-style environment for trading\n",
        "2. **DQN with Dueling Architecture**: Separate value and advantage streams\n",
        "3. **Double DQN**: Reduces overestimation bias in Q-learning\n",
        "4. **Experience Replay**: Breaks correlations for stable training\n",
        "5. **Policy Analysis**: Understanding what the agent learned\n",
        "\n",
        "### Key Insights:\n",
        "- DQN can learn adaptive allocation strategies\n",
        "- Agent learns to switch strategies based on market conditions\n",
        "- Transaction costs influence the frequency of rebalancing\n",
        "\n",
        "### Extensions to Try:\n",
        "- Continuous action spaces with DDPG/SAC\n",
        "- Add more market features (sentiment, macro indicators)\n",
        "- Use real historical data\n",
        "- Implement PPO for more stable learning"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
