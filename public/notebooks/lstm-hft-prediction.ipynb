{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM & Transformer for High-Frequency Trading Prediction\n",
        "\n",
        "This notebook demonstrates how to use deep learning sequence models for predicting short-term price movements.\n",
        "\n",
        "## Key Concepts\n",
        "- **LSTM (Long Short-Term Memory)**: Recurrent network that captures temporal dependencies\n",
        "- **Transformer**: Attention-based model that can look at all timesteps simultaneously\n",
        "- **Features**: OHLCV data, technical indicators, order book imbalance\n",
        "\n",
        "## Requirements\n",
        "```bash\n",
        "pip install torch numpy pandas matplotlib yfinance ta\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate Synthetic High-Frequency Data\n",
        "\n",
        "For demonstration, we generate synthetic tick data with realistic properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_hft_data(n_ticks=100000, tick_size=0.01):\n",
        "    \"\"\"Generate synthetic tick data with microstructure effects\"\"\"\n",
        "    # Initialize\n",
        "    prices = [100.0]\n",
        "    volumes = []\n",
        "    bid_ask_spreads = []\n",
        "    order_imbalances = []\n",
        "    \n",
        "    # Regime switching volatility\n",
        "    volatilities = [0.0001, 0.0003, 0.0005]  # Low, medium, high vol regimes\n",
        "    regime = 0\n",
        "    \n",
        "    for i in range(n_ticks - 1):\n",
        "        # Regime switching (Markov)\n",
        "        if np.random.random() < 0.001:\n",
        "            regime = np.random.choice([0, 1, 2], p=[0.5, 0.35, 0.15])\n",
        "        \n",
        "        vol = volatilities[regime]\n",
        "        \n",
        "        # Price change with mean reversion and momentum\n",
        "        momentum = 0.1 * (prices[-1] - np.mean(prices[-100:])) if len(prices) > 100 else 0\n",
        "        noise = np.random.randn() * vol * prices[-1]\n",
        "        price_change = -0.0001 * momentum + noise\n",
        "        \n",
        "        # Round to tick size\n",
        "        new_price = round((prices[-1] + price_change) / tick_size) * tick_size\n",
        "        new_price = max(new_price, 1.0)  # Floor at $1\n",
        "        prices.append(new_price)\n",
        "        \n",
        "        # Volume (correlated with volatility)\n",
        "        base_volume = 100 + 50 * regime\n",
        "        volume = int(np.random.exponential(base_volume))\n",
        "        volumes.append(volume)\n",
        "        \n",
        "        # Bid-ask spread (wider in high vol)\n",
        "        spread = tick_size * (1 + regime + np.random.exponential(0.5))\n",
        "        bid_ask_spreads.append(spread)\n",
        "        \n",
        "        # Order imbalance (predictive signal)\n",
        "        imbalance = np.clip(np.random.randn() * 0.3 + 0.2 * np.sign(price_change), -1, 1)\n",
        "        order_imbalances.append(imbalance)\n",
        "    \n",
        "    # Add last volume etc\n",
        "    volumes.append(volumes[-1])\n",
        "    bid_ask_spreads.append(bid_ask_spreads[-1])\n",
        "    order_imbalances.append(order_imbalances[-1])\n",
        "    \n",
        "    df = pd.DataFrame({\n",
        "        'price': prices,\n",
        "        'volume': volumes,\n",
        "        'spread': bid_ask_spreads,\n",
        "        'imbalance': order_imbalances\n",
        "    })\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Generate data\n",
        "df = generate_synthetic_hft_data(100000)\n",
        "print(f\"Generated {len(df)} ticks\")\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_features(df, lookback_windows=[5, 10, 20, 50]):\n",
        "    \"\"\"Create features for prediction\"\"\"\n",
        "    features = pd.DataFrame(index=df.index)\n",
        "    \n",
        "    # Price returns at different scales\n",
        "    features['return_1'] = df['price'].pct_change(1)\n",
        "    features['return_5'] = df['price'].pct_change(5)\n",
        "    features['return_10'] = df['price'].pct_change(10)\n",
        "    \n",
        "    # Moving averages ratio\n",
        "    for w in lookback_windows:\n",
        "        ma = df['price'].rolling(w).mean()\n",
        "        features[f'ma_ratio_{w}'] = df['price'] / ma - 1\n",
        "    \n",
        "    # Volatility at different scales\n",
        "    for w in lookback_windows:\n",
        "        features[f'volatility_{w}'] = df['price'].pct_change().rolling(w).std()\n",
        "    \n",
        "    # Volume features\n",
        "    features['volume_ma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
        "    features['volume_std'] = df['volume'].rolling(20).std() / df['volume'].rolling(20).mean()\n",
        "    \n",
        "    # Spread features\n",
        "    features['spread'] = df['spread']\n",
        "    features['spread_ma_ratio'] = df['spread'] / df['spread'].rolling(20).mean()\n",
        "    \n",
        "    # Order imbalance\n",
        "    features['imbalance'] = df['imbalance']\n",
        "    features['imbalance_ma'] = df['imbalance'].rolling(10).mean()\n",
        "    \n",
        "    # Target: direction of next tick (up=1, down=0)\n",
        "    features['target'] = (df['price'].shift(-1) > df['price']).astype(int)\n",
        "    \n",
        "    # Drop NaN rows\n",
        "    features = features.dropna()\n",
        "    \n",
        "    return features\n",
        "\n",
        "features = create_features(df)\n",
        "print(f\"Features shape: {features.shape}\")\n",
        "print(f\"\\nFeature columns: {list(features.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Sequences for LSTM/Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_sequences(features, seq_length=50, train_ratio=0.8):\n",
        "    \"\"\"Prepare sequences for sequence models\"\"\"\n",
        "    # Separate features and target\n",
        "    X = features.drop('target', axis=1).values\n",
        "    y = features['target'].values\n",
        "    \n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    # Create sequences\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X_scaled) - seq_length):\n",
        "        X_seq.append(X_scaled[i:i+seq_length])\n",
        "        y_seq.append(y[i+seq_length-1])  # Predict direction at end of sequence\n",
        "    \n",
        "    X_seq = np.array(X_seq)\n",
        "    y_seq = np.array(y_seq)\n",
        "    \n",
        "    # Train/test split (no shuffle to maintain temporal order)\n",
        "    split_idx = int(len(X_seq) * train_ratio)\n",
        "    \n",
        "    X_train = torch.tensor(X_seq[:split_idx], dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_seq[:split_idx], dtype=torch.long)\n",
        "    X_test = torch.tensor(X_seq[split_idx:], dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_seq[split_idx:], dtype=torch.long)\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test, scaler\n",
        "\n",
        "SEQ_LENGTH = 50\n",
        "X_train, y_train, X_test, y_test, scaler = prepare_sequences(features, SEQ_LENGTH)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "print(f\"\\nClass distribution (train): {np.bincount(y_train.numpy())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim, hidden_dim, num_layers,\n",
        "            batch_first=True, dropout=dropout, bidirectional=True\n",
        "        )\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # LSTM encoding\n",
        "        lstm_out, _ = self.lstm(x)  # (batch, seq, hidden*2)\n",
        "        \n",
        "        # Attention weights\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        \n",
        "        # Weighted sum\n",
        "        context = (lstm_out * attn_weights).sum(dim=1)  # (batch, hidden*2)\n",
        "        \n",
        "        # Classification\n",
        "        return self.fc(context)\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train.shape[2]\n",
        "lstm_model = LSTMClassifier(input_dim, hidden_dim=64).to(device)\n",
        "print(f\"LSTM parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        \n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model, nhead, dim_feedforward=128, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Project to d_model dimensions\n",
        "        x = self.input_projection(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        \n",
        "        # Transformer encoding\n",
        "        x = self.transformer(x)\n",
        "        \n",
        "        # Use last timestep for classification\n",
        "        x = x[:, -1, :]\n",
        "        \n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize model\n",
        "transformer_model = TransformerClassifier(input_dim, d_model=64).to(device)\n",
        "print(f\"Transformer parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, X_train, y_train, X_test, y_test, epochs=50, batch_size=256, lr=0.001):\n",
        "    \"\"\"Train a model and return training history\"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "    \n",
        "    X_train_dev = X_train.to(device)\n",
        "    y_train_dev = y_train.to(device)\n",
        "    X_test_dev = X_test.to(device)\n",
        "    y_test_dev = y_test.to(device)\n",
        "    \n",
        "    train_losses = []\n",
        "    test_accs = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        \n",
        "        # Mini-batch training\n",
        "        indices = torch.randperm(len(X_train_dev))\n",
        "        for i in range(0, len(X_train_dev), batch_size):\n",
        "            batch_idx = indices[i:i+batch_size]\n",
        "            X_batch = X_train_dev[batch_idx]\n",
        "            y_batch = y_train_dev[batch_idx]\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "        \n",
        "        avg_loss = total_loss / n_batches\n",
        "        train_losses.append(avg_loss)\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(X_test_dev)\n",
        "            test_preds = test_outputs.argmax(dim=1)\n",
        "            test_acc = (test_preds == y_test_dev).float().mean().item()\n",
        "            test_accs.append(test_acc)\n",
        "        \n",
        "        scheduler.step(avg_loss)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Test Acc = {test_acc:.4f}\")\n",
        "    \n",
        "    return train_losses, test_accs\n",
        "\n",
        "# Train LSTM\n",
        "print(\"Training LSTM...\")\n",
        "lstm_losses, lstm_accs = train_model(lstm_model, X_train, y_train, X_test, y_test, epochs=30)\n",
        "\n",
        "# Train Transformer\n",
        "print(\"\\nTraining Transformer...\")\n",
        "transformer_losses, transformer_accs = train_model(transformer_model, X_train, y_train, X_test, y_test, epochs=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Training loss\n",
        "axes[0].plot(lstm_losses, label='LSTM', linewidth=2)\n",
        "axes[0].plot(transformer_losses, label='Transformer', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Training Loss')\n",
        "axes[0].set_title('Training Loss Comparison')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Test accuracy\n",
        "axes[1].plot(lstm_accs, label='LSTM', linewidth=2)\n",
        "axes[1].plot(transformer_accs, label='Transformer', linewidth=2)\n",
        "axes[1].axhline(y=0.5, color='gray', linestyle='--', label='Random baseline')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Test Accuracy')\n",
        "axes[1].set_title('Test Accuracy Comparison')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal Test Accuracy:\")\n",
        "print(f\"  LSTM: {lstm_accs[-1]:.4f}\")\n",
        "print(f\"  Transformer: {transformer_accs[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Detailed Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    \"\"\"Detailed evaluation of a model\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X_test_dev = X_test.to(device)\n",
        "        outputs = model(X_test_dev)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "    \n",
        "    y_true = y_test.numpy()\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{model_name} Evaluation\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(classification_report(y_true, preds, target_names=['Down', 'Up']))\n",
        "    \n",
        "    # Confidence analysis\n",
        "    high_conf_mask = (probs > 0.6) | (probs < 0.4)\n",
        "    if high_conf_mask.sum() > 0:\n",
        "        high_conf_acc = accuracy_score(y_true[high_conf_mask], preds[high_conf_mask])\n",
        "        print(f\"High confidence predictions ({high_conf_mask.sum()} samples): {high_conf_acc:.4f} accuracy\")\n",
        "    \n",
        "    return probs, preds\n",
        "\n",
        "lstm_probs, lstm_preds = evaluate_model(lstm_model, X_test, y_test, \"LSTM\")\n",
        "transformer_probs, transformer_preds = evaluate_model(transformer_model, X_test, y_test, \"Transformer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Simulated Trading Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_trading(y_true, y_pred, probs, threshold=0.55):\n",
        "    \"\"\"Simulate trading based on predictions\"\"\"\n",
        "    pnl = []\n",
        "    trades = []\n",
        "    \n",
        "    for i in range(len(y_true)):\n",
        "        conf = abs(probs[i] - 0.5) * 2  # Confidence 0-1\n",
        "        \n",
        "        # Only trade on high confidence predictions\n",
        "        if max(probs[i], 1-probs[i]) > threshold:\n",
        "            predicted_direction = 1 if probs[i] > 0.5 else -1\n",
        "            actual_direction = 1 if y_true[i] == 1 else -1\n",
        "            \n",
        "            # P&L: +1 if correct, -1 if wrong\n",
        "            trade_pnl = predicted_direction * actual_direction\n",
        "            pnl.append(trade_pnl)\n",
        "            trades.append(i)\n",
        "    \n",
        "    return np.array(pnl), trades\n",
        "\n",
        "# Simulate trading\n",
        "lstm_pnl, lstm_trades = simulate_trading(y_test.numpy(), lstm_preds, lstm_probs, threshold=0.55)\n",
        "transformer_pnl, transformer_trades = simulate_trading(y_test.numpy(), transformer_preds, transformer_probs, threshold=0.55)\n",
        "\n",
        "# Plot cumulative P&L\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "if len(lstm_pnl) > 0:\n",
        "    axes[0].plot(np.cumsum(lstm_pnl), label='LSTM', linewidth=2)\n",
        "if len(transformer_pnl) > 0:\n",
        "    axes[0].plot(np.cumsum(transformer_pnl), label='Transformer', linewidth=2)\n",
        "axes[0].axhline(y=0, color='gray', linestyle='--')\n",
        "axes[0].set_xlabel('Trade #')\n",
        "axes[0].set_ylabel('Cumulative P&L (units)')\n",
        "axes[0].set_title('Simulated Trading P&L (High Confidence Only)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Prediction confidence distribution\n",
        "axes[1].hist(lstm_probs, bins=50, alpha=0.5, label='LSTM', density=True)\n",
        "axes[1].hist(transformer_probs, bins=50, alpha=0.5, label='Transformer', density=True)\n",
        "axes[1].axvline(x=0.5, color='gray', linestyle='--')\n",
        "axes[1].set_xlabel('Predicted Probability (Up)')\n",
        "axes[1].set_ylabel('Density')\n",
        "axes[1].set_title('Prediction Confidence Distribution')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTrading Summary:\")\n",
        "print(f\"  LSTM: {len(lstm_trades)} trades, Win rate: {(lstm_pnl > 0).mean():.2%}, Total P&L: {lstm_pnl.sum():.0f}\")\n",
        "print(f\"  Transformer: {len(transformer_trades)} trades, Win rate: {(transformer_pnl > 0).mean():.2%}, Total P&L: {transformer_pnl.sum():.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **LSTM with Attention**: Bidirectional LSTM with attention mechanism for sequence classification\n",
        "2. **Transformer**: Self-attention based model for capturing long-range dependencies\n",
        "3. **Feature Engineering**: Technical indicators, volume, spread, and order imbalance features\n",
        "4. **Trading Simulation**: Using model confidence to filter trades\n",
        "\n",
        "### Key Insights:\n",
        "- Both models can learn patterns in tick data beyond random chance\n",
        "- High-confidence predictions tend to be more accurate\n",
        "- Transaction costs and latency are critical in real HFT (not modeled here)\n",
        "\n",
        "### Extensions to Try:\n",
        "- Add limit order book features (L2 data)\n",
        "- Use real tick data from Polygon.io or similar\n",
        "- Implement execution simulation with realistic slippage\n",
        "- Try temporal fusion transformers or other architectures"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
