{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph Neural Networks for Stock Relationship Modeling\n",
        "\n",
        "This notebook demonstrates how to use Graph Neural Networks (GNNs) to model relationships between stocks and predict returns based on network effects.\n",
        "\n",
        "## Key Concepts\n",
        "- **Graph Construction**: Build stock network from correlations/supply chains\n",
        "- **Graph Convolutional Network (GCN)**: Aggregate information from connected stocks\n",
        "- **Node Classification/Regression**: Predict individual stock returns using network context\n",
        "\n",
        "## Requirements\n",
        "```bash\n",
        "pip install torch torch-geometric numpy pandas matplotlib networkx\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import torch_geometric, provide fallback message if not available\n",
        "try:\n",
        "    from torch_geometric.nn import GCNConv, GATConv\n",
        "    from torch_geometric.data import Data\n",
        "    TORCH_GEOMETRIC_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"torch_geometric not installed. Install with:\")\n",
        "    print(\"pip install torch-geometric\")\n",
        "    TORCH_GEOMETRIC_AVAILABLE = False\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate Synthetic Stock Network Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_stock_network(n_stocks=50, n_days=500):\n",
        "    \"\"\"\n",
        "    Generate synthetic stock data with network effects.\n",
        "    \n",
        "    Returns:\n",
        "        returns: Stock returns matrix (n_days, n_stocks)\n",
        "        adjacency: Adjacency matrix (n_stocks, n_stocks)\n",
        "        sectors: Sector assignments for each stock\n",
        "    \"\"\"\n",
        "    # Create sector structure\n",
        "    n_sectors = 5\n",
        "    stocks_per_sector = n_stocks // n_sectors\n",
        "    sectors = np.repeat(range(n_sectors), stocks_per_sector)\n",
        "    if len(sectors) < n_stocks:\n",
        "        sectors = np.concatenate([sectors, [n_sectors-1] * (n_stocks - len(sectors))])\n",
        "    \n",
        "    # Generate sector factors\n",
        "    sector_factors = np.random.randn(n_days, n_sectors) * 0.02\n",
        "    \n",
        "    # Generate market factor\n",
        "    market_factor = np.random.randn(n_days) * 0.015\n",
        "    \n",
        "    # Generate stock-specific returns with network spillover\n",
        "    returns = np.zeros((n_days, n_stocks))\n",
        "    \n",
        "    # Adjacency matrix based on sector membership + random connections\n",
        "    adjacency = np.zeros((n_stocks, n_stocks))\n",
        "    \n",
        "    for i in range(n_stocks):\n",
        "        for j in range(i+1, n_stocks):\n",
        "            # Same sector = higher connection probability\n",
        "            if sectors[i] == sectors[j]:\n",
        "                prob = 0.6\n",
        "            else:\n",
        "                prob = 0.1\n",
        "            \n",
        "            if np.random.random() < prob:\n",
        "                weight = np.random.uniform(0.3, 1.0)\n",
        "                adjacency[i, j] = weight\n",
        "                adjacency[j, i] = weight\n",
        "    \n",
        "    # Normalize adjacency (row-stochastic)\n",
        "    adj_norm = adjacency / (adjacency.sum(axis=1, keepdims=True) + 1e-10)\n",
        "    \n",
        "    # Generate returns with network spillover\n",
        "    for t in range(n_days):\n",
        "        # Base returns: market + sector + idiosyncratic\n",
        "        base_returns = (\n",
        "            market_factor[t] +\n",
        "            sector_factors[t, sectors] +\n",
        "            np.random.randn(n_stocks) * 0.025\n",
        "        )\n",
        "        \n",
        "        # Add network spillover from previous day\n",
        "        if t > 0:\n",
        "            spillover = adj_norm @ returns[t-1] * 0.3\n",
        "            base_returns += spillover\n",
        "        \n",
        "        returns[t] = base_returns\n",
        "    \n",
        "    return returns, adjacency, sectors\n",
        "\n",
        "# Generate data\n",
        "N_STOCKS = 50\n",
        "N_DAYS = 500\n",
        "\n",
        "returns, adjacency, sectors = generate_stock_network(N_STOCKS, N_DAYS)\n",
        "\n",
        "print(f\"Returns shape: {returns.shape}\")\n",
        "print(f\"Adjacency shape: {adjacency.shape}\")\n",
        "print(f\"Number of edges: {(adjacency > 0).sum() // 2}\")\n",
        "print(f\"Sectors: {np.bincount(sectors)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Visualize Stock Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create NetworkX graph for visualization\n",
        "G = nx.from_numpy_array(adjacency)\n",
        "\n",
        "# Node colors by sector\n",
        "sector_colors = plt.cm.Set1(np.linspace(0, 1, len(np.unique(sectors))))\n",
        "node_colors = [sector_colors[s] for s in sectors]\n",
        "\n",
        "# Layout\n",
        "pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=200, alpha=0.8)\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5)\n",
        "nx.draw_networkx_labels(G, pos, font_size=6)\n",
        "\n",
        "# Legend\n",
        "for i, color in enumerate(sector_colors[:len(np.unique(sectors))]):\n",
        "    plt.scatter([], [], c=[color], label=f'Sector {i}', s=100)\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(f'Stock Network ({N_STOCKS} stocks, {(adjacency > 0).sum() // 2} edges)')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Network statistics\n",
        "print(f\"\\nNetwork Statistics:\")\n",
        "print(f\"  Average degree: {np.mean(list(dict(G.degree()).values())):.2f}\")\n",
        "print(f\"  Clustering coefficient: {nx.average_clustering(G):.4f}\")\n",
        "print(f\"  Density: {nx.density(G):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Graph Data for GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_graph_dataset(returns, adjacency, lookback=20, horizon=1):\n",
        "    \"\"\"\n",
        "    Create graph dataset for GNN training.\n",
        "    \n",
        "    Features per node:\n",
        "    - Historical returns (lookback days)\n",
        "    - Volatility\n",
        "    - Momentum\n",
        "    \n",
        "    Target: Next day return\n",
        "    \"\"\"\n",
        "    n_days, n_stocks = returns.shape\n",
        "    \n",
        "    # Create edge index from adjacency\n",
        "    edge_index = []\n",
        "    edge_weights = []\n",
        "    for i in range(n_stocks):\n",
        "        for j in range(n_stocks):\n",
        "            if adjacency[i, j] > 0:\n",
        "                edge_index.append([i, j])\n",
        "                edge_weights.append(adjacency[i, j])\n",
        "    \n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).T\n",
        "    edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
        "    \n",
        "    # Create features and targets for each time step\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    \n",
        "    for t in range(lookback, n_days - horizon):\n",
        "        # Features: lookback returns, volatility, momentum\n",
        "        hist_returns = returns[t-lookback:t, :]  # (lookback, n_stocks)\n",
        "        volatility = hist_returns.std(axis=0)\n",
        "        momentum = hist_returns.sum(axis=0)\n",
        "        recent_return = hist_returns[-1, :]\n",
        "        \n",
        "        # Stack features\n",
        "        features = np.column_stack([\n",
        "            recent_return,\n",
        "            volatility,\n",
        "            momentum\n",
        "        ])\n",
        "        \n",
        "        # Target: next day return\n",
        "        target = returns[t + horizon, :]\n",
        "        \n",
        "        X_list.append(features)\n",
        "        y_list.append(target)\n",
        "    \n",
        "    X = np.array(X_list)  # (n_samples, n_stocks, n_features)\n",
        "    y = np.array(y_list)  # (n_samples, n_stocks)\n",
        "    \n",
        "    return X, y, edge_index, edge_weights\n",
        "\n",
        "# Create dataset\n",
        "LOOKBACK = 20\n",
        "X, y, edge_index, edge_weights = create_graph_dataset(returns, adjacency, lookback=LOOKBACK)\n",
        "\n",
        "# Train/test split\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"Edge index shape: {edge_index.shape}\")\n",
        "print(f\"Number of features per node: {X_train.shape[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Graph Convolutional Network (GCN) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_GEOMETRIC_AVAILABLE:\n",
        "    class StockGCN(nn.Module):\n",
        "        \"\"\"Graph Convolutional Network for stock return prediction\"\"\"\n",
        "        def __init__(self, input_dim, hidden_dim=32, n_layers=2, dropout=0.2):\n",
        "            super().__init__()\n",
        "            \n",
        "            self.convs = nn.ModuleList()\n",
        "            self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "            for _ in range(n_layers - 1):\n",
        "                self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            \n",
        "            self.fc = nn.Linear(hidden_dim, 1)\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        def forward(self, x, edge_index, edge_weight=None):\n",
        "            # x: (n_nodes, input_dim)\n",
        "            # edge_index: (2, n_edges)\n",
        "            \n",
        "            for conv in self.convs[:-1]:\n",
        "                x = conv(x, edge_index, edge_weight)\n",
        "                x = F.relu(x)\n",
        "                x = self.dropout(x)\n",
        "            \n",
        "            x = self.convs[-1](x, edge_index, edge_weight)\n",
        "            x = F.relu(x)\n",
        "            \n",
        "            # Output prediction for each node\n",
        "            out = self.fc(x).squeeze(-1)\n",
        "            return out\n",
        "\n",
        "    # Initialize model\n",
        "    input_dim = X_train.shape[2]\n",
        "    model = StockGCN(input_dim, hidden_dim=32, n_layers=2).to(device)\n",
        "    edge_index = edge_index.to(device)\n",
        "    edge_weights = edge_weights.to(device)\n",
        "    \n",
        "    print(f\"GCN parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "else:\n",
        "    print(\"Skipping GCN model (torch_geometric not available)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train GCN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_GEOMETRIC_AVAILABLE:\n",
        "    def train_gcn(model, X_train, y_train, X_test, y_test, edge_index, edge_weights,\n",
        "                  epochs=100, lr=0.01):\n",
        "        \"\"\"Train GCN model\"\"\"\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "        criterion = nn.MSELoss()\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
        "        \n",
        "        X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "        y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "        X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "        y_test_t = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "        \n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            \n",
        "            # Train on each time step\n",
        "            for t in range(len(X_train_t)):\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                x = X_train_t[t]  # (n_stocks, n_features)\n",
        "                y = y_train_t[t]  # (n_stocks,)\n",
        "                \n",
        "                y_pred = model(x, edge_index, edge_weights)\n",
        "                loss = criterion(y_pred, y)\n",
        "                \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "            \n",
        "            avg_train_loss = total_loss / len(X_train_t)\n",
        "            train_losses.append(avg_train_loss)\n",
        "            \n",
        "            # Evaluate\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                test_loss = 0\n",
        "                for t in range(len(X_test_t)):\n",
        "                    y_pred = model(X_test_t[t], edge_index, edge_weights)\n",
        "                    test_loss += criterion(y_pred, y_test_t[t]).item()\n",
        "                avg_test_loss = test_loss / len(X_test_t)\n",
        "                test_losses.append(avg_test_loss)\n",
        "            \n",
        "            scheduler.step(avg_test_loss)\n",
        "            \n",
        "            if (epoch + 1) % 20 == 0:\n",
        "                print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.6f}, Test Loss = {avg_test_loss:.6f}\")\n",
        "        \n",
        "        return train_losses, test_losses\n",
        "\n",
        "    # Train\n",
        "    print(\"Training GCN...\")\n",
        "    train_losses, test_losses = train_gcn(\n",
        "        model, X_train, y_train, X_test, y_test, \n",
        "        edge_index, edge_weights, epochs=100\n",
        "    )\n",
        "\n",
        "    # Plot training\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(train_losses, label='Train')\n",
        "    plt.plot(test_losses, label='Test')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('GCN Training Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping training (torch_geometric not available)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compare with Baseline (No Network Information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPBaseline(nn.Module):\n",
        "    \"\"\"Simple MLP without graph structure\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x).squeeze(-1)\n",
        "        return x\n",
        "\n",
        "# Train baseline MLP\n",
        "mlp = MLPBaseline(X_train.shape[2], hidden_dim=32).to(device)\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
        "\n",
        "mlp_train_losses = []\n",
        "mlp_test_losses = []\n",
        "\n",
        "print(\"Training MLP baseline...\")\n",
        "for epoch in range(100):\n",
        "    mlp.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for t in range(len(X_train_t)):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = mlp(X_train_t[t])\n",
        "        loss = criterion(y_pred, y_train_t[t])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    mlp_train_losses.append(total_loss / len(X_train_t))\n",
        "    \n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = sum(criterion(mlp(X_test_t[t]), y_test_t[t]).item() for t in range(len(X_test_t)))\n",
        "        mlp_test_losses.append(test_loss / len(X_test_t))\n",
        "    \n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Test Loss = {mlp_test_losses[-1]:.6f}\")\n",
        "\n",
        "print(f\"\\nFinal Test MSE (MLP): {mlp_test_losses[-1]:.6f}\")\n",
        "if TORCH_GEOMETRIC_AVAILABLE:\n",
        "    print(f\"Final Test MSE (GCN): {test_losses[-1]:.6f}\")\n",
        "    improvement = (mlp_test_losses[-1] - test_losses[-1]) / mlp_test_losses[-1] * 100\n",
        "    print(f\"GCN improvement: {improvement:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Analyze Learned Node Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TORCH_GEOMETRIC_AVAILABLE:\n",
        "    def get_node_embeddings(model, x, edge_index, edge_weight):\n",
        "        \"\"\"Extract node embeddings from intermediate layer\"\"\"\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Pass through first layers\n",
        "            for conv in model.convs[:-1]:\n",
        "                x = conv(x, edge_index, edge_weight)\n",
        "                x = F.relu(x)\n",
        "            x = model.convs[-1](x, edge_index, edge_weight)\n",
        "        return x.cpu().numpy()\n",
        "\n",
        "    # Get embeddings for a sample time step\n",
        "    sample_x = torch.tensor(X_test[0], dtype=torch.float32).to(device)\n",
        "    embeddings = get_node_embeddings(model, sample_x, edge_index, edge_weights)\n",
        "\n",
        "    # Reduce to 2D for visualization\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(\n",
        "        embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
        "        c=sectors, cmap='Set1', s=100, alpha=0.7\n",
        "    )\n",
        "    plt.colorbar(scatter, label='Sector')\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.title('GCN Node Embeddings (PCA)')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Check if stocks in same sector cluster together\n",
        "    from sklearn.metrics import silhouette_score\n",
        "    sil_score = silhouette_score(embeddings, sectors)\n",
        "    print(f\"\\nSector clustering quality (silhouette score): {sil_score:.4f}\")\n",
        "else:\n",
        "    print(\"Skipping embedding analysis (torch_geometric not available)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prediction Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_predictions(model, X_test, y_test, edge_index, edge_weights, use_gcn=True):\n",
        "    \"\"\"Evaluate model predictions\"\"\"\n",
        "    model.eval()\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    \n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for t in range(len(X_test_t)):\n",
        "            if use_gcn and TORCH_GEOMETRIC_AVAILABLE:\n",
        "                preds = model(X_test_t[t], edge_index, edge_weights)\n",
        "            else:\n",
        "                preds = model(X_test_t[t])\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_targets.append(y_test[t])\n",
        "    \n",
        "    return np.array(all_preds), np.array(all_targets)\n",
        "\n",
        "# Get predictions\n",
        "mlp_preds, targets = evaluate_predictions(mlp, X_test, y_test, edge_index, edge_weights, use_gcn=False)\n",
        "\n",
        "if TORCH_GEOMETRIC_AVAILABLE:\n",
        "    gcn_preds, _ = evaluate_predictions(model, X_test, y_test, edge_index, edge_weights, use_gcn=True)\n",
        "\n",
        "# Calculate metrics\n",
        "def calc_metrics(preds, targets):\n",
        "    mse = np.mean((preds - targets) ** 2)\n",
        "    mae = np.mean(np.abs(preds - targets))\n",
        "    # Direction accuracy\n",
        "    direction_acc = np.mean((preds > 0) == (targets > 0))\n",
        "    return mse, mae, direction_acc\n",
        "\n",
        "mlp_mse, mlp_mae, mlp_dir = calc_metrics(mlp_preds.flatten(), targets.flatten())\n",
        "print(f\"\\nMLP Results:\")\n",
        "print(f\"  MSE: {mlp_mse:.6f}\")\n",
        "print(f\"  MAE: {mlp_mae:.6f}\")\n",
        "print(f\"  Direction Accuracy: {mlp_dir:.2%}\")\n",
        "\n",
        "if TORCH_GEOMETRIC_AVAILABLE:\n",
        "    gcn_mse, gcn_mae, gcn_dir = calc_metrics(gcn_preds.flatten(), targets.flatten())\n",
        "    print(f\"\\nGCN Results:\")\n",
        "    print(f\"  MSE: {gcn_mse:.6f}\")\n",
        "    print(f\"  MAE: {gcn_mae:.6f}\")\n",
        "    print(f\"  Direction Accuracy: {gcn_dir:.2%}\")\n",
        "\n",
        "# Visualize predictions vs actual\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scatter plot\n",
        "sample = np.random.choice(len(targets.flatten()), 1000, replace=False)\n",
        "axes[0].scatter(targets.flatten()[sample], mlp_preds.flatten()[sample], alpha=0.3, s=10, label='MLP')\n",
        "if TORCH_GEOMETRIC_AVAILABLE:\n",
        "    axes[0].scatter(targets.flatten()[sample], gcn_preds.flatten()[sample], alpha=0.3, s=10, label='GCN')\n",
        "axes[0].plot([-0.1, 0.1], [-0.1, 0.1], 'r--', label='Perfect')\n",
        "axes[0].set_xlabel('Actual Return')\n",
        "axes[0].set_ylabel('Predicted Return')\n",
        "axes[0].set_title('Predictions vs Actual')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Time series for one stock\n",
        "stock_idx = 0\n",
        "axes[1].plot(targets[:50, stock_idx], label='Actual', alpha=0.8)\n",
        "axes[1].plot(mlp_preds[:50, stock_idx], label='MLP', alpha=0.8)\n",
        "if TORCH_GEOMETRIC_AVAILABLE:\n",
        "    axes[1].plot(gcn_preds[:50, stock_idx], label='GCN', alpha=0.8)\n",
        "axes[1].set_xlabel('Time Step')\n",
        "axes[1].set_ylabel('Return')\n",
        "axes[1].set_title(f'Stock {stock_idx} Return Predictions')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Graph Construction**: Built stock network from sector membership and random connections\n",
        "2. **Graph Convolutional Network**: GCN model that aggregates information from connected stocks\n",
        "3. **Network Effects**: Stocks can be predicted better by considering neighbors\n",
        "4. **Node Embeddings**: GCN learns representations that capture sector structure\n",
        "\n",
        "### Key Insights:\n",
        "- GCN can leverage network structure to improve predictions\n",
        "- Learned embeddings cluster by sector (stocks with similar connections have similar representations)\n",
        "- Network effects in returns can be captured by message passing\n",
        "\n",
        "### Extensions to Try:\n",
        "- Use real stock data and build network from correlations or supply chain relationships\n",
        "- Try Graph Attention Networks (GAT) for learnable edge weights\n",
        "- Add temporal dynamics with T-GCN (Temporal Graph Convolutional Network)\n",
        "- Experiment with different graph construction methods (correlation threshold, kNN, etc.)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
