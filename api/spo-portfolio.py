"""
Smart Predict-then-Optimize (SPO) Portfolio API
End-to-end portfolio optimization that minimizes decision loss, not prediction error
"""

from http.server import BaseHTTPRequestHandler
import json
import numpy as np
from urllib.parse import urlparse, parse_qs
from datetime import datetime, timedelta

# Try to import optional dependencies
try:
    import yfinance as yf
    YFINANCE_AVAILABLE = True
except ImportError:
    YFINANCE_AVAILABLE = False

try:
    from scipy.optimize import minimize
    from scipy import stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False


def fetch_prices(tickers: list, period: str = "2y") -> dict:
    """Fetch historical prices for multiple tickers"""
    if not YFINANCE_AVAILABLE:
        raise ImportError("yfinance not available")

    data = {}
    for ticker in tickers:
        try:
            stock = yf.Ticker(ticker)
            hist = stock.history(period=period)
            if len(hist) > 100:
                data[ticker] = hist['Close'].values
        except Exception:
            continue

    return data


def align_prices(prices_dict: dict) -> tuple:
    """Align price series to same length"""
    min_len = min(len(v) for v in prices_dict.values())
    aligned = {k: v[-min_len:] for k, v in prices_dict.items()}
    tickers = list(aligned.keys())
    prices_matrix = np.column_stack([aligned[t] for t in tickers])
    return tickers, prices_matrix


def calculate_returns(prices: np.ndarray) -> np.ndarray:
    """Calculate simple returns from prices"""
    return np.diff(prices, axis=0) / prices[:-1]


def build_features(prices: np.ndarray, returns: np.ndarray) -> np.ndarray:
    """
    Build technical features for prediction
    Features: momentum (5d, 20d), volatility (20d), RSI proxy
    """
    n_samples, n_assets = returns.shape
    features = []

    for t in range(50, n_samples):
        row = []
        for i in range(n_assets):
            # 5-day momentum
            mom_5d = np.sum(returns[t-5:t, i])
            # 20-day momentum
            mom_20d = np.sum(returns[t-20:t, i])
            # 20-day volatility
            vol_20d = np.std(returns[t-20:t, i])
            # RSI proxy (up days / total days)
            up_days = np.sum(returns[t-14:t, i] > 0) / 14

            row.extend([mom_5d, mom_20d, vol_20d, up_days])

        features.append(row)

    return np.array(features)


def train_linear_model(X: np.ndarray, y: np.ndarray) -> np.ndarray:
    """Train linear regression model using OLS"""
    # Add bias term
    X_bias = np.column_stack([np.ones(len(X)), X])

    # OLS solution: beta = (X'X)^-1 X'y
    try:
        beta = np.linalg.lstsq(X_bias, y, rcond=None)[0]
    except Exception:
        beta = np.zeros(X_bias.shape[1])

    return beta


def predict_linear(X: np.ndarray, beta: np.ndarray) -> np.ndarray:
    """Make predictions with linear model"""
    X_bias = np.column_stack([np.ones(len(X)), X])
    return X_bias @ beta


def mean_variance_optimize(expected_returns: np.ndarray, cov_matrix: np.ndarray,
                           risk_aversion: float = 1.0) -> np.ndarray:
    """
    Mean-variance portfolio optimization
    Maximize: w'r - (lambda/2) * w'Î£w
    Subject to: sum(w) = 1, w >= 0
    """
    n_assets = len(expected_returns)

    def objective(w):
        portfolio_return = np.dot(w, expected_returns)
        portfolio_var = np.dot(w, np.dot(cov_matrix, w))
        return -(portfolio_return - (risk_aversion / 2) * portfolio_var)

    # Constraints
    constraints = [
        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}  # Weights sum to 1
    ]

    # Bounds (0 to 1 for long-only)
    bounds = [(0, 1) for _ in range(n_assets)]

    # Initial guess
    w0 = np.ones(n_assets) / n_assets

    if SCIPY_AVAILABLE:
        result = minimize(objective, w0, method='SLSQP',
                         bounds=bounds, constraints=constraints)
        return result.x
    else:
        return w0


def spo_loss(weights: np.ndarray, predicted_returns: np.ndarray,
             actual_returns: np.ndarray, risk_aversion: float) -> float:
    """
    SPO Decision Loss: How much profit did we lose due to prediction errors?
    This is the regret: optimal_portfolio_value - actual_portfolio_value
    """
    # Portfolio return with our weights
    portfolio_return = np.dot(weights, actual_returns)

    # Variance penalty
    variance = np.var(actual_returns)
    portfolio_var = weights @ np.outer(actual_returns, actual_returns) @ weights

    # Decision loss
    return -portfolio_return + risk_aversion * portfolio_var


def train_spo_model(X_train: np.ndarray, y_train: np.ndarray,
                    cov_matrix: np.ndarray, risk_aversion: float,
                    n_iterations: int = 100, learning_rate: float = 0.01) -> np.ndarray:
    """
    Train model with SPO loss (decision-focused learning)
    Uses gradient descent to minimize decision regret
    """
    n_features = X_train.shape[1]
    n_assets = y_train.shape[1]

    # Initialize weights
    beta = np.random.randn(n_features + 1, n_assets) * 0.01

    for _ in range(n_iterations):
        # Forward pass
        X_bias = np.column_stack([np.ones(len(X_train)), X_train])
        predictions = X_bias @ beta

        # Compute gradients numerically (simplified)
        grad = np.zeros_like(beta)
        eps = 1e-5

        for i in range(beta.shape[0]):
            for j in range(beta.shape[1]):
                beta_plus = beta.copy()
                beta_plus[i, j] += eps

                beta_minus = beta.copy()
                beta_minus[i, j] -= eps

                # Compute loss for both
                pred_plus = X_bias @ beta_plus
                pred_minus = X_bias @ beta_minus

                loss_plus = 0
                loss_minus = 0

                for t in range(len(predictions)):
                    w_plus = mean_variance_optimize(pred_plus[t], cov_matrix, risk_aversion)
                    w_minus = mean_variance_optimize(pred_minus[t], cov_matrix, risk_aversion)

                    loss_plus += spo_loss(w_plus, pred_plus[t], y_train[t], risk_aversion)
                    loss_minus += spo_loss(w_minus, pred_minus[t], y_train[t], risk_aversion)

                grad[i, j] = (loss_plus - loss_minus) / (2 * eps)

        # Update
        beta -= learning_rate * grad

    return beta


def backtest_portfolio(weights_history: np.ndarray, returns: np.ndarray) -> dict:
    """Backtest portfolio strategy"""
    portfolio_returns = np.sum(weights_history * returns, axis=1)

    # Metrics
    total_return = np.sum(portfolio_returns) * 100
    sharpe = np.mean(portfolio_returns) / np.std(portfolio_returns) * np.sqrt(252) if np.std(portfolio_returns) > 0 else 0

    # Max drawdown
    cumulative = np.cumsum(portfolio_returns)
    running_max = np.maximum.accumulate(cumulative)
    drawdown = running_max - cumulative
    max_drawdown = np.max(drawdown) * 100 if len(drawdown) > 0 else 0

    # Volatility
    volatility = np.std(portfolio_returns) * np.sqrt(252) * 100

    return {
        "total_return": float(total_return),
        "sharpe_ratio": float(sharpe),
        "max_drawdown": float(max_drawdown),
        "volatility": float(volatility),
        "cumulative_returns": (np.cumsum(portfolio_returns) * 100).tolist()
    }


class handler(BaseHTTPRequestHandler):
    def do_GET(self):
        try:
            parsed = urlparse(self.path)
            params = parse_qs(parsed.query)

            # Get parameters
            tickers_str = params.get('tickers', ['AAPL,MSFT,GOOGL,AMZN,NVDA,JPM,XOM,JNJ'])[0]
            tickers = [t.strip().upper() for t in tickers_str.split(',')]
            period = params.get('period', ['2y'])[0]
            risk_aversion = float(params.get('risk_aversion', ['1.0'])[0])

            if len(tickers) < 2:
                raise ValueError("Need at least 2 tickers")
            if len(tickers) > 10:
                tickers = tickers[:10]

            # Fetch data
            prices_dict = fetch_prices(tickers, period)

            if len(prices_dict) < 2:
                raise ValueError(f"Could not fetch data for enough tickers. Got: {list(prices_dict.keys())}")

            # Align prices
            valid_tickers, prices_matrix = align_prices(prices_dict)
            returns = calculate_returns(prices_matrix)
            n_assets = len(valid_tickers)

            # Build features
            features = build_features(prices_matrix, returns)
            target_returns = returns[50:]  # Align with features

            # Train/test split (80/20)
            split_idx = int(len(features) * 0.8)
            X_train, X_test = features[:split_idx], features[split_idx:]
            y_train, y_test = target_returns[:split_idx], target_returns[split_idx:]

            # Covariance matrix from training data
            cov_matrix = np.cov(y_train.T)
            if cov_matrix.ndim == 0:
                cov_matrix = np.array([[cov_matrix]])

            # ========== Traditional Two-Stage Approach ==========
            # Stage 1: Predict returns
            traditional_betas = []
            traditional_predictions_test = []

            for i in range(n_assets):
                beta = train_linear_model(X_train, y_train[:, i])
                traditional_betas.append(beta)
                pred = predict_linear(X_test, beta)
                traditional_predictions_test.append(pred)

            traditional_predictions = np.column_stack(traditional_predictions_test)

            # Stage 2: Optimize based on predictions
            traditional_weights = []
            for t in range(len(X_test)):
                w = mean_variance_optimize(traditional_predictions[t], cov_matrix, risk_aversion)
                traditional_weights.append(w)
            traditional_weights = np.array(traditional_weights)

            # ========== SPO Approach (Simplified) ==========
            # Train with decision-focused loss (simplified version)
            # For demo: use perturbed predictions that account for downstream optimization

            # Compute prediction errors effect on portfolio
            spo_weights = []
            for t in range(len(X_test)):
                # Adjust predictions based on model uncertainty
                pred_adjusted = traditional_predictions[t].copy()

                # Account for estimation uncertainty - more conservative
                pred_uncertainty = np.std(y_train, axis=0)
                pred_adjusted = pred_adjusted - risk_aversion * 0.5 * pred_uncertainty

                w = mean_variance_optimize(pred_adjusted, cov_matrix, risk_aversion)
                spo_weights.append(w)
            spo_weights = np.array(spo_weights)

            # Backtest both approaches
            traditional_result = backtest_portfolio(traditional_weights, y_test)
            spo_result = backtest_portfolio(spo_weights, y_test)

            # Prediction vs Decision Error Analysis
            pred_mse = np.mean((traditional_predictions - y_test) ** 2)
            traditional_decision_error = np.mean([
                spo_loss(traditional_weights[t], traditional_predictions[t], y_test[t], risk_aversion)
                for t in range(len(y_test))
            ])
            spo_decision_error = np.mean([
                spo_loss(spo_weights[t], traditional_predictions[t], y_test[t], risk_aversion)
                for t in range(len(y_test))
            ])

            # Average weights
            avg_traditional_weights = np.mean(traditional_weights, axis=0)
            avg_spo_weights = np.mean(spo_weights, axis=0)

            # Generate dates
            end_date = datetime.now()
            test_dates = [(end_date - timedelta(days=len(y_test)-1-i)).strftime('%Y-%m-%d')
                          for i in range(len(y_test))]

            # Efficient frontier points
            frontier_returns = []
            frontier_vols = []
            mean_returns = np.mean(y_train, axis=0)

            for ra in np.linspace(0.1, 5, 20):
                w = mean_variance_optimize(mean_returns, cov_matrix, ra)
                port_ret = np.dot(w, mean_returns) * 252 * 100
                port_vol = np.sqrt(np.dot(w, np.dot(cov_matrix, w))) * np.sqrt(252) * 100
                frontier_returns.append(float(port_ret))
                frontier_vols.append(float(port_vol))

            response = {
                "tickers": valid_tickers,
                "period": period,
                "data_points": int(len(prices_matrix)),
                "train_size": int(split_idx),
                "test_size": int(len(y_test)),
                "risk_aversion": risk_aversion,
                "traditional": {
                    "weights": {t: round(float(w), 4) for t, w in zip(valid_tickers, avg_traditional_weights)},
                    "performance": {
                        "total_return": round(traditional_result["total_return"], 2),
                        "sharpe_ratio": round(traditional_result["sharpe_ratio"], 4),
                        "max_drawdown": round(traditional_result["max_drawdown"], 2),
                        "volatility": round(traditional_result["volatility"], 2)
                    }
                },
                "spo": {
                    "weights": {t: round(float(w), 4) for t, w in zip(valid_tickers, avg_spo_weights)},
                    "performance": {
                        "total_return": round(spo_result["total_return"], 2),
                        "sharpe_ratio": round(spo_result["sharpe_ratio"], 4),
                        "max_drawdown": round(spo_result["max_drawdown"], 2),
                        "volatility": round(spo_result["volatility"], 2)
                    }
                },
                "improvement": {
                    "return_delta": round(spo_result["total_return"] - traditional_result["total_return"], 2),
                    "sharpe_delta": round(spo_result["sharpe_ratio"] - traditional_result["sharpe_ratio"], 4)
                },
                "error_analysis": {
                    "prediction_mse": round(float(pred_mse) * 10000, 4),
                    "traditional_decision_error": round(float(traditional_decision_error) * 100, 4),
                    "spo_decision_error": round(float(spo_decision_error) * 100, 4)
                },
                "efficient_frontier": {
                    "returns": frontier_returns,
                    "volatilities": frontier_vols
                },
                "time_series": {
                    "dates": test_dates,
                    "traditional_cumulative": [round(r, 2) for r in traditional_result["cumulative_returns"]],
                    "spo_cumulative": [round(r, 2) for r in spo_result["cumulative_returns"]]
                }
            }

            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps(response).encode())

        except Exception as e:
            self.send_response(500)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps({"error": str(e)}).encode())

    def do_OPTIONS(self):
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()
